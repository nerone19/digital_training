{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e17724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen/qwen3-30b-a3b\"\n",
    "tailscale_server = \"https://desktop-3oeimac.tail3b962f.ts.net\"\n",
    "chat_completion_api = tailscale_server + \"/api/v0/chat/completions\"\n",
    "embedding_model = \"text-embedding-qwen3-embedding-8b@q5_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e390badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title not found for https://www.youtube.com/watch?v=NyjXMMBPvSA\n",
      "Error loading the transcript for https://www.youtube.com/watch?v=NyjXMMBPvSA\n",
      "title not found for https://www.youtube.com/watch?v=mQENVePdT5A&t=6s\n",
      "Error loading the transcript for https://www.youtube.com/watch?v=mQENVePdT5A&t=6s\n",
      "[{'url': 'https://www.youtube.com/watch?v=NyjXMMBPvSA', 'title': None, 'transcript': None}, {'url': 'https://www.youtube.com/watch?v=mQENVePdT5A&t=6s', 'title': None, 'transcript': None}]\n"
     ]
    }
   ],
   "source": [
    "# step 1: we look for existing transcript for the selected videos.\n",
    "import glob \n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "media_dir = \"./media/it\"\n",
    "loaded_videos = []\n",
    "for doc in glob.glob(pathname=f\"{media_dir}/*.txt\", recursive=True):\n",
    "    with open(doc, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        \n",
    "        for line in lines:\n",
    "            url = line.strip()\n",
    "            yt = YouTube(url)\n",
    "            try:\n",
    "                title = yt.title\n",
    "            except Exception:\n",
    "                title = None\n",
    "                print(f'title not found for {url}')\n",
    "\n",
    "            try:  \n",
    "                loader = YoutubeLoader.from_youtube_url(\n",
    "                    url, language=['en', 'it'], continue_on_failure=True\n",
    "                )\n",
    "            \n",
    "                transcript = loader.load()\n",
    "            except Exception:\n",
    "                print(f'Error loading the transcript for {url}')\n",
    "                transcript = None\n",
    "            loaded_videos.append({\"url\": url, \"title\": title, \"transcript\": transcript })\n",
    "            \n",
    "\n",
    "\n",
    "print(loaded_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "081cfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Callable, Dict, Iterator, Literal, Optional, Tuple, Union\n",
    "from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParserLocal\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.base import BaseBlobParser\n",
    "from langchain_community.document_loaders.parsers.audio import _get_audio_from_blob\n",
    "\n",
    "from langchain_community.document_loaders.blob_loaders import Blob\n",
    "\n",
    "from langchain_community.utils.openai import is_openai_v1\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "\n",
    "logger = logging.getLogger()\n",
    "class OpenAIWhisperParserLocalCustom(BaseBlobParser):\n",
    "    \"\"\"Transcribe and parse audio files with OpenAI Whisper model.\n",
    "\n",
    "    Audio transcription with OpenAI Whisper model locally from transformers.\n",
    "\n",
    "    Parameters:\n",
    "    device - device to use\n",
    "        NOTE: By default uses the gpu if available,\n",
    "        if you want to use cpu, please set device = \"cpu\"\n",
    "    lang_model - whisper model to use, for example \"openai/whisper-medium\"\n",
    "    forced_decoder_ids - id states for decoder in multilanguage model,\n",
    "        usage example:\n",
    "        from transformers import WhisperProcessor\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"french\",\n",
    "          task=\"transcribe\")\n",
    "        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"french\",\n",
    "        task=\"translate\")\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang_model: Optional[str] = None,\n",
    "        batch_size: int = 8,\n",
    "        chunk_length: int = 30,\n",
    "        forced_decoder_ids: Optional[Tuple[Dict]] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the parser.\n",
    "\n",
    "        Args:\n",
    "            device: device to use.\n",
    "            lang_model: whisper model to use, for example \"openai/whisper-medium\".\n",
    "              Defaults to None.\n",
    "            forced_decoder_ids: id states for decoder in a multilanguage model.\n",
    "              Defaults to None.\n",
    "            batch_size: batch size used for decoding\n",
    "              Defaults to 8.\n",
    "            chunk_length: chunk length used during inference.\n",
    "              Defaults to 30s.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"transformers package not found, please install it with \"\n",
    "                \"`pip install transformers`\"\n",
    "            )\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"torch package not found, please install it with `pip install torch`\"\n",
    "            )\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        self.batch_size = batch_size\n",
    "        model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "        )\n",
    "        model.to(self.device)\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=self.device,\n",
    "            chunk_length_s=chunk_length        )\n",
    "        \n",
    "        if forced_decoder_ids is not None:\n",
    "            try:\n",
    "                self.pipe.model.config.forced_decoder_ids = forced_decoder_ids\n",
    "            except Exception as exception_text:\n",
    "                logger.info(\n",
    "                    \"Unable to set forced_decoder_ids parameter for whisper model\"\n",
    "                    f\"Text of exception: {exception_text}\"\n",
    "                    \"Therefore whisper model will use default mode for decoder\"\n",
    "                )\n",
    "\n",
    "    def lazy_parse(self, blob: Blob) -> Iterator[Document]:\n",
    "        \"\"\"Lazily parse the blob.\"\"\"\n",
    "\n",
    "        try:\n",
    "            import librosa\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"librosa package not found, please install it with \"\n",
    "                \"`pip install librosa`\"\n",
    "            )\n",
    "\n",
    "        audio = _get_audio_from_blob(blob)\n",
    "\n",
    "        file_obj = io.BytesIO(audio.export(format=\"mp3\").read())\n",
    "\n",
    "        # Transcribe\n",
    "        print(f\"Transcribing part  new {blob.path}!\")  # noqa: T201\n",
    "\n",
    "        y, sr = librosa.load(file_obj, sr=16000)\n",
    "\n",
    "        prediction = self.pipe(y.copy(), batch_size=self.batch_size)[\"text\"]\n",
    "\n",
    "        yield Document(\n",
    "            page_content=prediction,\n",
    "            metadata={\"source\": blob.source},\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=NyjXMMBPvSA\n",
      "[youtube] NyjXMMBPvSA: Downloading webpage\n",
      "[youtube] NyjXMMBPvSA: Downloading tv client config\n",
      "[youtube] NyjXMMBPvSA: Downloading tv player API JSON\n",
      "[youtube] NyjXMMBPvSA: Downloading ios player API JSON\n",
      "[youtube] NyjXMMBPvSA: Downloading m3u8 information\n",
      "[info] NyjXMMBPvSA: Downloading 1 format(s): 140\n",
      "[download] ./test/🔥COSA FARE NELLE TRAZIONI SE HAI LE LEVE LUNGHE？🔥.m4a has already been downloaded\n",
      "[download] 100% of    3.77MiB\n",
      "[ExtractAudio] Not converting audio ./test/🔥COSA FARE NELLE TRAZIONI SE HAI LE LEVE LUNGHE？🔥.m4a; file is already in target format m4a\n",
      "Transcribing part  new test/🔥COSA FARE NELLE TRAZIONI SE HAI LE LEVE LUNGHE？🔥.m4a!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriele.local/Documents/repos/caption_videos/venv/lib64/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "#step 2: we load audio and convert them if no transcript is available yet.\n",
    "from langchain_community.document_loaders import YoutubeAudioLoader\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParserLocal\n",
    "from langchain_community.document_loaders.blob_loaders.file_system import FileSystemBlobLoader\n",
    "\n",
    "# model = whisper.load_model(\"turbo\")\n",
    "# result = model.transcribe(\"./test/🔥COME MIGLIORARE TUTTE LE HALF LAY？🔥.m4a\")\n",
    "# print(result[\"text\"])\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "urls_to_process = [\"https://www.youtube.com/watch?v=NyjXMMBPvSA\"]\n",
    "save_dir = './test'\n",
    "# for v in loaded_videos:\n",
    "    \n",
    "#     if v.get('transcript',None) is None:\n",
    "#         urls_to_process.append(v.get('url', None))\n",
    "        \n",
    "\n",
    "loader = GenericLoader(\n",
    "        YoutubeAudioLoader(\n",
    "            urls_to_process, \n",
    "            save_dir\n",
    "        ),\n",
    "        OpenAIWhisperParserLocalCustom())\n",
    "loader.load()\n",
    "    # print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a4d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "/home/gabriele.local/Documents/repos/caption_videos/venv/lib64/python3.10/site-packages/transformers/models/whisper/generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ciao ragazzi benvenuti sul mio canale youtube sulla mia pagina facebook oggi vi voglio parlare della flay soprattutto per quello che riguarda va bene nel front lever ok ma soprattutto nella planche vabbè anche la bandiera volendo ma anche nella verticale perché la flay è una variante così scomoda prima di continuare con il bellissimo argomento di questo video voglio invitarti ad ad attivare la campanellina, a lasciare un commento, a lasciare un like, solo così potete aiutarmi a far crescere questo canale che è sempre stato gratuito e che ho sempre voluto coltivare con la mia passione. L'aumento delle visualizzazioni, l'aumento degli iscritti, quanto può diffondere questo canale è solo merito vostro. Oltre a questo vi invito a seguirmi sul mio canale Instagram dove condivido i risultati dei miei allievi perché io ci tengo ad essere conosciuto per il risultato dei miei allievi sono importantissimi e sotto questi video vado a condividere anche molte tips e molti suggerimenti visitate il mio sito dove ci sono articoli gratuiti le demo dei miei video corsi che poi se vi piacciono potete anche acquistare e dopo dal menu contattami potete anche ricevere tutte le informazioni sui prezzi e le modalità per farvi seguire da me io seguo chi vuole imparare allora molte persone riferiscono di non riuscire a mantenere una linea completa che va dal busto fino al ginocchio passando per i fianchi perché tra virgolette non riescono ad estendere l'anca sentono di avere il culo basso allora il fatto è che la fley per come è fatta vi crea un'estensione dell'anca e una flessione del ginocchio nel complesso tutto questo porta ad una maggiore tensione sui biarticolari dell'anca quindi principalmente il retto del femore e se voi non avete abbastanza forza nei glutei per andare ad estendere l'anca e a flettere il ginocchio cosa che porta in allungamento questi biarticolari vi trovate ad avere il culo che scende ed erroneamente pensate che quella variante lì non sia vostra perché perché vi cade il culo e associate se vi cade il culo non è abbastanza forza per i dorsali quindi il mio consiglio qual è anzi sono due il primo prima di pensare che è un problema di forza provate a lavorare un pochino sull'allungamento dei flessori dell'anca biarticolari quindi qualsiasi stretching per lo psoas magari fatto in ginocchio dove andate a flettere il ginocchio nello stesso angolo della mezza completa o a flay come volete chiamarla e se anche questo non funziona perché è eccessiva la difficoltà che vi fate potete valutare di andare direttamente magari alla full utilizzando delle bande elastiche vi faccio però una nota quando voi contraete il culo per fare la flay non dovete diventare matti perché ricordatevi che i glutei sono anche dei rotatori esterni quindi voi potete trovarvi che nell'esigenza di contrarre i glutei vi trovate ad avere i piedi che fanno così verso l'esterno cioè il femore tende ad extra-rotare ed è una cosa naturale che succede anche estesa quindi ragazzi provate queste due cosine qua e poi mi dite che cosa succede e noi ci vediamo al prossimo video\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "result = pipe(\"./test/🔥COME MIGLIORARE TUTTE LE HALF LAY？🔥.m4a\",  return_timestamps=True)\n",
    "\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b94db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.10/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.10/site-packages (from ipywidgets) (8.37.0)\n",
      "Collecting jupyterlab_widgets~=3.0.15\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=4.0.14\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.10/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./venv/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.14.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.10/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "401b248d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b7d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
