{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77e17724",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"qwen/qwen3-30b-a3b\"\n",
    "tailscale_server = \"https://desktop-3oeimac.tail3b962f.ts.net\"\n",
    "chat_completion_api = tailscale_server + \"/api/v0/chat/completions\"\n",
    "embedding_model = \"text-embedding-qwen3-embedding-8b@q5_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e390badb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title not found for https://www.youtube.com/watch?v=NyjXMMBPvSA\n",
      "Error loading the transcript for https://www.youtube.com/watch?v=NyjXMMBPvSA\n",
      "title not found for https://www.youtube.com/watch?v=mQENVePdT5A&t=6s\n",
      "Error loading the transcript for https://www.youtube.com/watch?v=mQENVePdT5A&t=6s\n",
      "[{'url': 'https://www.youtube.com/watch?v=NyjXMMBPvSA', 'title': None, 'transcript': None}, {'url': 'https://www.youtube.com/watch?v=mQENVePdT5A&t=6s', 'title': None, 'transcript': None}]\n"
     ]
    }
   ],
   "source": [
    "# step 1: we look for existing transcript for the selected videos.\n",
    "import glob \n",
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "media_dir = \"./media/it\"\n",
    "loaded_videos = []\n",
    "for doc in glob.glob(pathname=f\"{media_dir}/*.txt\", recursive=True):\n",
    "    with open(doc, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "        \n",
    "        for line in lines:\n",
    "            url = line.strip()\n",
    "            yt = YouTube(url)\n",
    "            try:\n",
    "                title = yt.title\n",
    "            except Exception:\n",
    "                title = None\n",
    "                print(f'title not found for {url}')\n",
    "\n",
    "            try:  \n",
    "                loader = YoutubeLoader.from_youtube_url(\n",
    "                    url, language=['en', 'it'], continue_on_failure=True\n",
    "                )\n",
    "            \n",
    "                transcript = loader.load()\n",
    "            except Exception:\n",
    "                print(f'Error loading the transcript for {url}')\n",
    "                transcript = None\n",
    "            finally:\n",
    "                loaded_videos.append({\"url\": url, \"title\": title, \"transcript\": transcript })\n",
    "            \n",
    "\n",
    "\n",
    "print(loaded_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081cfb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import io\n",
    "from typing import Any, Callable, Dict, Iterator, Literal, Optional, Tuple, Union, List, Iterable\n",
    "from langchain_community.document_loaders.parsers.audio import OpenAIWhisperParserLocal\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders.base import BaseBlobParser\n",
    "from langchain_community.document_loaders.parsers.audio import _get_audio_from_blob\n",
    "\n",
    "from langchain_community.document_loaders.blob_loaders import Blob\n",
    "\n",
    "from langchain_community.utils.openai import is_openai_v1\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain_community.document_loaders.blob_loaders import (\n",
    "    BlobLoader,\n",
    "    FileSystemBlobLoader,\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "class OpenAIWhisperParserLocalCustom(OpenAIWhisperParserLocal):\n",
    "    \"\"\"Custom Parser for using turbo model from Whisper. Transcribe and parse audio files with OpenAI Whisper model.\n",
    "\n",
    "    Audio transcription with OpenAI Whisper model locally from transformers.\n",
    "\n",
    "    Parameters:\n",
    "    device - device to use\n",
    "        NOTE: By default uses the gpu if available,\n",
    "        if you want to use cpu, please set device = \"cpu\"\n",
    "    lang_model - whisper model to use, for example \"openai/whisper-medium\"\n",
    "    forced_decoder_ids - id states for decoder in multilanguage model,\n",
    "        usage example:\n",
    "        from transformers import WhisperProcessor\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"french\",\n",
    "          task=\"transcribe\")\n",
    "        forced_decoder_ids = WhisperProcessor.get_decoder_prompt_ids(language=\"french\",\n",
    "        task=\"translate\")\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        lang_model: Optional[str] = None,\n",
    "        batch_size: int = 8,\n",
    "        chunk_length: int = 30,\n",
    "        forced_decoder_ids: Optional[Tuple[Dict]] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the parser.\n",
    "\n",
    "        Args:\n",
    "            device: device to use.\n",
    "            lang_model: whisper model to use, for example \"openai/whisper-medium\".\n",
    "              Defaults to None.\n",
    "            forced_decoder_ids: id states for decoder in a multilanguage model.\n",
    "              Defaults to None.\n",
    "            batch_size: batch size used for decoding\n",
    "              Defaults to 8.\n",
    "            chunk_length: chunk length used during inference.\n",
    "              Defaults to 30s.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"transformers package not found, please install it with \"\n",
    "                \"`pip install transformers`\"\n",
    "            )\n",
    "        try:\n",
    "            import torch\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"torch package not found, please install it with `pip install torch`\"\n",
    "            )\n",
    "\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        self.batch_size = batch_size\n",
    "        model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "        model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "            model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True\n",
    "        )\n",
    "        model.to(self.device)\n",
    "\n",
    "        processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "        self.pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model,\n",
    "            tokenizer=processor.tokenizer,\n",
    "            feature_extractor=processor.feature_extractor,\n",
    "            torch_dtype=torch_dtype,\n",
    "            device=self.device,\n",
    "            chunk_length_s=chunk_length        )\n",
    "        \n",
    "        if forced_decoder_ids is not None:\n",
    "            try:\n",
    "                self.pipe.model.config.forced_decoder_ids = forced_decoder_ids\n",
    "            except Exception as exception_text:\n",
    "                logger.info(\n",
    "                    \"Unable to set forced_decoder_ids parameter for whisper model\"\n",
    "                    f\"Text of exception: {exception_text}\"\n",
    "                    \"Therefore whisper model will use default mode for decoder\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cade9ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/shorts/QtJNskFMZpY\n",
      "[youtube] QtJNskFMZpY: Downloading webpage\n",
      "[youtube] QtJNskFMZpY: Downloading tv client config\n",
      "[youtube] QtJNskFMZpY: Downloading tv player API JSON\n",
      "[youtube] QtJNskFMZpY: Downloading ios player API JSON\n",
      "[youtube] QtJNskFMZpY: Downloading m3u8 information\n",
      "[info] QtJNskFMZpY: Downloading 1 format(s): 140\n",
      "[download] Destination: test\\Are Push-Ups Worth Doing On Rings？.m4a\n",
      "[download] 100% of  639.19KiB in 00:00:00 at 1.97MiB/s   \n",
      "[FixupM4a] Correcting container of \"test\\Are Push-Ups Worth Doing On Rings？.m4a\"\n",
      "[ExtractAudio] Not converting audio test\\Are Push-Ups Worth Doing On Rings？.m4a; file is already in target format m4a\n",
      "[youtube] Extracting URL: https://www.youtube.com/shorts/OxQzgN8i6QQ\n",
      "[youtube] OxQzgN8i6QQ: Downloading webpage\n",
      "[youtube] OxQzgN8i6QQ: Downloading tv client config\n",
      "[youtube] OxQzgN8i6QQ: Downloading tv player API JSON\n",
      "[youtube] OxQzgN8i6QQ: Downloading ios player API JSON\n",
      "[youtube] OxQzgN8i6QQ: Downloading m3u8 information\n",
      "[info] OxQzgN8i6QQ: Downloading 1 format(s): 140\n",
      "[download] Destination: test\\Calisthenics Workouts Explained.m4a\n",
      "[download] 100% of  897.13KiB in 00:00:00 at 2.43MiB/s     \n",
      "[FixupM4a] Correcting container of \"test\\Calisthenics Workouts Explained.m4a\"\n",
      "[ExtractAudio] Not converting audio test\\Calisthenics Workouts Explained.m4a; file is already in target format m4a\n",
      "Transcribing part test\\Are Push-Ups Worth Doing On Rings？.m4a!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriele Martinero\\Documents\\digital_training\\venv\\lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:604: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing part test\\Calisthenics Workouts Explained.m4a!\n",
      "Are Push-Ups Worth Doing On Rings？.m4a is removed\n",
      "Calisthenics Workouts Explained.m4a is removed\n"
     ]
    }
   ],
   "source": [
    "#step 2: we load audio and convert them if no transcript is available yet. We assume that all the urls we have need still to be processed.\n",
    "from langchain_community.document_loaders import YoutubeAudioLoader\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "import os\n",
    "\n",
    "# # todo: this needs to be filled in with urls not yet processed.\n",
    "urls_to_process = [\"https://www.youtube.com/shorts/QtJNskFMZpY\", \"https://www.youtube.com/shorts/OxQzgN8i6QQ\"]\n",
    "save_dir = './test'\n",
    "\n",
    "loader = GenericLoader(\n",
    "        YoutubeAudioLoader(\n",
    "            urls_to_process, \n",
    "            save_dir\n",
    "        ),\n",
    "        OpenAIWhisperParserLocalCustom())\n",
    "docs = loader.load()\n",
    "\n",
    "# we keep the folder clean to avoid the re-processing of previously downloaded videos (look at YoutubeAudioLoader lazy_load method to understand.)\n",
    "for filename in os.listdir(save_dir):\n",
    "   file_path = os.path.join(save_dir, filename)\n",
    "   if os.path.isfile(file_path):\n",
    "      os.remove(file_path)\n",
    "      print(filename, \"is removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "052a4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# step 3: split each transcription into chunks and insert them in DB\n",
    "processed_transcript_files = []\n",
    "for doc in docs:\n",
    "    # we save the content of the transcript inside a txt file\n",
    "    transcript_file_path = os.path.join(media_dir, doc.metadata.get(\"source\").split(\"\\\\\")[-1]) \n",
    "    processed_transcript_files.append(transcript_file_path)\n",
    "    with open(transcript_file_path, \"w\") as file:\n",
    "        # doc is the output of the Whisper Models\n",
    "        file.write(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b94db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcript_summarizer': 'You are part of a system whose focus is giving practical strategies for increasing performance in workouts relating to bodyweight and gymnastics. \\n <ROLE> \\n Your primary role is to assist users by summing up transcripts videos given by the user as input, focusing on finding practices which have been useful in workout plans. Summing up means that the generated summary should not be longer of the provided input. Therefore try being more concise, not too verbose and straight to the point. \\n </ROLE>'}\n"
     ]
    }
   ],
   "source": [
    "# step 4: we chunck and summarize each document\n",
    "import json\n",
    "with open(\"./prompt_catalog.json\") as catalog_file:\n",
    "    prompt_catalog = json.load(catalog_file)\n",
    "    \n",
    "print(prompt_catalog.get('system'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1973d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reasoning_model_prompt(prompt:str,no_think=True):\n",
    "    if no_think:\n",
    "        return prompt + \" /no_think\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604a230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "file_chunks = []\n",
    "for doc in docs: \n",
    "    # in the future we expect to know the context window of the model from somewhere else.\n",
    "    text_splitter = TokenTextSplitter(chunk_size=3200, chunk_overlap=40)\n",
    "    file_chunks.append(text_splitter.split_text(doc.page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "20074294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b248d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 15\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize the LLM pointing to your LM Studio server\u001b[39;00m\n\u001b[0;32m      6\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatOpenAI(\n\u001b[0;32m      7\u001b[0m     base_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtailscale_server\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# LM Studio server URL\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlm-studio\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Can be any string for local models\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,  \u001b[38;5;66;03m# Can be any string for local models\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     13\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     14\u001b[0m     SystemMessage(json_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranscript_summarizer\u001b[39m\u001b[38;5;124m'\u001b[39m)),\n\u001b[1;32m---> 15\u001b[0m     HumanMessage(\u001b[43mformat_reasoning_model_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_chunks\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m     16\u001b[0m ]\n\u001b[0;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(messages)\n",
      "Cell \u001b[1;32mIn[55], line 3\u001b[0m, in \u001b[0;36mformat_reasoning_model_prompt\u001b[1;34m(prompt, no_think)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mformat_reasoning_model_prompt\u001b[39m(prompt:\u001b[38;5;28mstr\u001b[39m,no_think\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m no_think:\n\u001b[1;32m----> 3\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m /no_think\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Initialize the LLM pointing to your LM Studio server\n",
    "llm = ChatOpenAI(\n",
    "    base_url=f\"{tailscale_server}/v1\",  # LM Studio server URL\n",
    "    api_key=\"lm-studio\",  # Can be any string for local models\n",
    "    model=model,  # Can be any string for local models\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(json_data.get('system').get('transcript_summarizer')),\n",
    "    HumanMessage(format_reasoning_model_prompt(docs[0])),\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b7d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (2.0.41)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (9.1.2)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (0.4.4)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (2.2.6)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (2.10.1)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (0.3.26)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (0.3.68)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain_community) (2.32.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
      "Requirement already satisfied: anyio in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: You must give at least one requirement to install (see \"pip help install\")\n",
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yt_dlp in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (2025.6.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (4.14.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (1.7.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: packaging in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in c:\\users\\gabriele martinero\\documents\\digital_training\\venv\\lib\\site-packages (0.25.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytube"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pytube\n",
      "Successfully installed pytube-15.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community\n",
    "!pip install\n",
    "!pip install yt_dlp\n",
    "!pip install librosa\n",
    "!pip install pydub\n",
    "!pip install pytube\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f2a580e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are Push-Ups Worth Doing On Rings？.m4a'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata.get(\"source\").split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "136540ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Are Push-Ups Worth Doing On Rings？.m4a'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata.get(\"source\").split(\"\\\\\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246bf79c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
